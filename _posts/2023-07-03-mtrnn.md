# Multiple-Timescales Recurrent Neural Networks (MTRNN)

The MTRNN is a network architecture capable of learning long time dependencies more effectively than an Elman network.
This architecture is described in [Yamashita and Tani 2010](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000220#s4), 
__Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment__.

This is very similar to a cascade of Elman or Jordan recurrent networks, with the key difference that the networks are now considered
to operate in continuous time, and the units in the higher levels have slower (larger) time constants.

I made a simple adaptation of the network in Yamshita and Tani 2010 for the problem of learning traces (sequences of 2D coordinates).
The basic architecture is in the next figure.

![mtrnn adaptation](/assets/mtrnn.png)

The input to the network is a sequence of $(x, y)$ coordinates, and the output is a prediction of the coordinates $(x', y')$  at the next
time step. L0 is a fully-connected layer, using $tanh$ nonlinearities in this implementation. L1 is a linear layer. H0 and H1 are Elman RNN
cells with fast and slow timescales, respectively.

Tracing circles, triangles, and figure eights was not particularly challenging, but the circles swithching location after a few
revolutions were trickier.

The first trace of this kind that I trained to learn with the MTRNN consisted of 5 circular revolutions on the left, followed by
5 revolutions on the right, then back to the left, and so on. I used layer sizes of 10, 40, 8 for L0, H0, and H1, respectively.
The time constant for H0 was 3, and the one for H1 was 40.

Initially the network was only learning ...


This prompted me to increase the number of epochs. After 10000 epochs the error reached a very small value (~8e-05), and the results
can be seen below:

![traces_long_train](/assets/switch_circle5_circle5_long_train.png)
![many initial conditions](/assets/switch_circle5_circle5_long_train2.png)

It can be observed that the network learnd to trace multiple revolutions on the left, before finally settling on the limit-cycle
attractor on the right. This is still not the desired pattern.

Presumably, the time dependency was still too long to be remembered given the time constants in the network. If this is the case,
the network will be able to handle a side-switching trace with shorter time dependencies. The next trace to learn was therefore
using only 2.5 revolutions before switching to the other side. This is what happened:

![fewer revolutions](/assets/switch_circle2_circle2_4200.png)

Below is what the network produces starting from 16 different initial conditions.  
![no cigar](/assets/switch_circle2_circle2_4200_ics.png)


These don't seem to be the attractors we're looking for. Next step was to increase the time constants in the network, from 3 in H0 and 40 in H1 to 4 in H0 and 80 in H1.
