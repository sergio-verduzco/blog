# Multiple-Timescales Recurrent Neural Networks (MTRNN)

The MTRNN is a network architecture capable of learning long time dependencies more effectively than an Elman network.
This architecture is described in [Yamashita and Tani 2010](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000220#s4), 
__Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment__.

...

Tracing circles, triangles, and figure eights was not particularly challenging, but the circles swithching location after a few
revolutions were trickier.

The first trace of this kind that I trained to learn with the MTRNN consisted of 5 circular revolutions on the left, followed by
5 revolutions on the right, then back to the left, and so on. I used layer sizes of 10, 40, 8 for L0, H0, and H1, respectively.
The time constant for H0 was 3, and the one for H1 was 40.

Initially the network was only learning ...


This prompted me to increase the number of epochs. After 10000 epochs the error reached a very small value (~8e-05), and the results
can be seen below:

![traces_long_train](/assets/switch_circle5_circle5_long_train.png)
![many initial conditions](/assets/switch_circle5_circle5_long_train2.png)

It can be observed that the network learnd to trace multiple revolutions on the left, before finally settling on the limit-cycle
attractor on the right. This is still not the desired pattern.

Presumably, the time dependency was still too long to be remembered given the time constants in the network. If this is the case,
the network will be able to handle a side-switching trace with shorter time dependencies. The next trace to learn was therefore
using only 2.5 revolutions before switching to the other side.

