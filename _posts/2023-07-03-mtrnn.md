---
title: Multiple-Timescales Recurrent Neural Networks (MTRNNs)
date: 2023-07-03
---

## Introduction

This post is a continuation of my previous post on [Elman networks](https://sergio-verduzco.github.io/2023/07/02/elman_rnn.html).

The MTRNN is a network architecture capable of learning long time dependencies more effectively than an Elman network.
This architecture is described in [Yamashita and Tani 2010](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000220#s4), 
__Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment__.

This is very similar to a cascade of Elman or Jordan recurrent networks, with the key difference that the networks are now considered
to operate in continuous time, and the units in the higher levels have slower (larger) time constants.

I made a simple adaptation of the network in Yamshita and Tani 2010 for the problem of learning traces (sequences of 2D coordinates).
The basic architecture is in the next figure.

![mtrnn adaptation](/assets/mtrnn.png)

As in the Elman RNN post, the input to the network is a sequence of $(x, y)$ coordinates, and the output is a prediction of the coordinates $(x', y')$  at the next
time step. L0 is a fully-connected layer, using $tanh$ nonlinearities in this implementation. L1 is a linear layer. H0 and H1 are Elman RNN
cells with fast and slow timescales, respectively.

A difference with the Yamashita and Tani 2010 network is that in here outputs come from a linear layer, rather than a softmax layer.
Moreover, H0 and H1 each consist of a single layer, and they update after applying the $tanh$ nonlinearity. In other words, if
$\mathbf{h}_i$ denotes the activation vector of the units in hidden layer $i$, the update steps are described by:

$$\mathbf{h}_i^* = \tanh \left( \mathbf{W_i I_i} \right)$$

$$\mathbf{h}_i \leftarrow \left( 1 - \frac{1}{\tau_i} \right) \mathbf{h}_i + \frac{1}{\tau_i} \mathbf{h}_i^*$$

Where $\mathbf{W}_i$ and $\mathbf{I}_i$ are respectively the weight matrix and input vector for hidden layer $i$.
As a curious note, initially I made a mistake when wrting the equations to update the activity, so the update was:
$\mathbf{h}_i \leftarrow \left( 1 - \frac{1}{\tau_i} \right) \mathbf{h}_i^*$.
The network with this update could still learn to approximate circles, triangles, and with some work even figure eights. This is
probably because the network with these update equations is like two stacked Elman networks with some error in the
activation. It serves to illustrate a valuable lesson, so I threw this result in an Appendix at the bottom of this post.

Going back to the network with the correct equations, I used layer sizes of 10, 40, 8 for L0, H0, and H1, respectively.
The time constant for H0 was 4, and the one for H1 was 80. The optimizer was Adam with time constant 3e-4.


## Results

Like the Elman network, the MTRNN could learn to trace circles, triangles, and infinity symbols. I'll spare the readers from those results.


## Appendix: Incorrect equations, but so-so results

Before I realized that my MTRNN had the wrong equations I tried to teach it a pattern consisting of 5 circular revolutions on the left, followed by5 revolutions on the right, then back to the left, and so on. 

The network is still not what 


This prompted me to increase the number of epochs. After 10000 epochs the error reached a very small value (~8e-05), and the results
can be seen below:

![traces_long_train](/assets/switch_circle5_circle5_long_train.png)
![many initial conditions](/assets/switch_circle5_circle5_long_train2.png)

It can be observed that the network learnd to trace multiple revolutions on the left, before finally settling on the limit-cycle
attractor on the right. This is still not the desired pattern.

Presumably, the time dependency was still too long to be remembered given the time constants in the network. If this is the case,
the network will be able to handle a side-switching trace with shorter time dependencies. The next trace to learn was therefore
using only 2.5 revolutions before switching to the other side. This is what happened:

![fewer revolutions](/assets/switch_circle2_circle2_4200.png)

Below is what the network produces starting from 16 different initial conditions.  
![no cigar](/assets/switch_circle2_circle2_4200_ics.png)


These don't seem to be the attractors we're looking for. Next step was to increase the time constants in the network, from 3 in H0 and 40 in H1 to 4 in H0 and 80 in H1.
