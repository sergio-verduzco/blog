<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://sergio-verduzco.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sergio-verduzco.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-09-05T10:34:48+09:00</updated><id>https://sergio-verduzco.github.io/feed.xml</id><title type="html">Sergio Verduzco</title><subtitle>Neural networks and whatnot. </subtitle><entry><title type="html">Migrating from Roam to Logseq with lots of uploaded files</title><link href="https://sergio-verduzco.github.io/blog/2023/roam_migration/" rel="alternate" type="text/html" title="Migrating from Roam to Logseq with lots of uploaded files"/><published>2023-09-02T00:00:00+09:00</published><updated>2023-09-02T00:00:00+09:00</updated><id>https://sergio-verduzco.github.io/blog/2023/roam_migration</id><content type="html" xml:base="https://sergio-verduzco.github.io/blog/2023/roam_migration/"><![CDATA[<p>Taking notes is a basic aspect of my job as a researcher and as a student. More than that, for a long time I have wanted these notes to be a tool for discovering relations between concepts, leading to new insights. Using a collection of .txt files arranged by date or by subject does not quite cut it, so I’ve been searching for the right note-taking tools.</p> <p>Over the years I’ve gone from using a MySQL database to mindmaps (mostly <a href="https://docs.freeplane.org/">Freeplane</a>) to <a href="https://tiddlywiki.com/">TiddlyWiki</a>. When I got to <a href="https://evernote.com/">Evernote</a> I felt things were starting to come together, but something was still missing.</p> <p>Back in 2020 I started using <a href="https://roamresearch.com/">Roam Research</a>, and my note-taking needs were almost fulfilled. But not quite. Roam is subscription software, not open-source, and stores your data remotely. I’m against all that, but Roam treated me well. So well, that for 3 years I was in denial, paying every month.</p> <p>But behold the goodness of <a href="https://logseq.com/">Logseq</a>!</p> <p>Logseq provides all the functionality I need from Roam, and more. It is free, open-source, and locally stored. The only thing that stopped me was the pain of migrating 3 years worth of notes. Luckily, <a href="https://hub.logseq.com/getting-started/uQdEHALJo7RWnDLLLP7uux/how-to-switch-from-roam-research-to-logseq/epbNMUYPWBSjxfrog8v2sH">migrating from Roam Research to Logseq</a> was not terrible. Still, it had a couple of challenges. A bothersome one was that I had uploaded a large number of images and PDF files into my Roam graph. When I exported the graph from Roam and imported it to Logseq all those files were still in some remote server.</p> <p>Luckily, my Logseq graph is stored as Markdown files, which can be easily manipulated. I wrote a Python script (in a <a href="https://jupyter.org/">Jupyter notebook</a>) to download all external PDF and image files in a Logseq graph to a local folder, and change the links accordingly in the Markdown files.</p> <p>The script is <a href="https://github.com/sergio-verduzco/lsad/tree/main">here</a>. Perhaps it will be useful to someone out there.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Taking notes is a basic aspect of my job as a researcher and as a student. More than that, for a long time I have wanted these notes to be a tool for discovering relations between concepts, leading to new insights. Using a collection of .txt files arranged by date or by subject does not quite cut it, so I’ve been searching for the right note-taking tools.]]></summary></entry><entry><title type="html">PV-RNN has the weirdest bugs</title><link href="https://sergio-verduzco.github.io/blog/2023/pvrnn/" rel="alternate" type="text/html" title="PV-RNN has the weirdest bugs"/><published>2023-08-15T00:00:00+09:00</published><updated>2023-08-15T00:00:00+09:00</updated><id>https://sergio-verduzco.github.io/blog/2023/pvrnn</id><content type="html" xml:base="https://sergio-verduzco.github.io/blog/2023/pvrnn/"><![CDATA[<p>PV-RNN is an architecture that evolved from VRNN, addressing some of its limitations.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Obtaining A1, A2 with BPTT
A_loss = self.min_loss + 1.
iter = 0
while loss &gt; self.min_loss and iter &lt; self.max_iter:
    A_optim.zero_grad()
    # self.zero_grad()
    iter += 1
    # predict the x sequence
    x_hat = self.fast_generate_from_posterior(A1, A2)
    # loss
    A_loss = self.mse_loss(x, x_hat)
    # backpropagation
    A_loss.backward()
    # update A1, A2
    A_optim.step()
# Obtaining predictions and posterior parameters with A1, A2
self.zero_grad()
x_hat = self.fast_generate_from_posterior(A1, A2, store=True)
return x_hat, A1, A2
</code></pre></div></div> <p>The latent state is a device to add stochasticity to the model so that the distribution associated with this stochasticity evolves through time. The prior network provides parameters to obtain the latent state $z_t^p$ given the current RNN hidden state $d_t$, and the value of the latent variables influence the next RNN state, from which the outputs are obtained. The network can thus produce outputs like a probabilistic finite-state machine. When you reach a point where a probabilistic state transition may happen this state is encoded in the RNN hidden state, from which the latent state is obtained. Transitions happening stochastically at this state cause prediction errors, which should cause the prior network to use more variance when producing the latent state.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[PV-RNN is an architecture that evolved from VRNN, addressing some of its limitations.]]></summary></entry><entry><title type="html">Variational Recurrent Neural Network (VRNN): not your regular RNN</title><link href="https://sergio-verduzco.github.io/blog/2023/vrnn/" rel="alternate" type="text/html" title="Variational Recurrent Neural Network (VRNN): not your regular RNN"/><published>2023-07-14T00:00:00+09:00</published><updated>2023-07-14T00:00:00+09:00</updated><id>https://sergio-verduzco.github.io/blog/2023/vrnn</id><content type="html" xml:base="https://sergio-verduzco.github.io/blog/2023/vrnn/"><![CDATA[<p>In this post I try to explain the ideas behind the Variational Recurrent Neural Network, and convey the experiences from implementing one with PyTorch.</p> <p>Rather than getting to the point, I relate the beginner errors I introduced into my program, which frustrated me for days. Also I write some of the rambling ideas I had during the process.</p> <h2 id="introduction">Introduction</h2> <p>In two <a href="https://sergio-verduzco.github.io/2023/07/02/elman_rnn.html">previous</a> <a href="https://sergio-verduzco.github.io/2023/07/03/mtrnn.html">posts</a> I played with RNNs for modeling sequential data. In particular, these networks could learn attractors that required significant memory of previous states.</p> <p>The RNNs I used, when recursively generating sequential data (when the output from time $t-1$ became the input at time $t$), constituted deterministic discrete dynamical systems. They do not have the ability to model a process that is inherently stochastic.</p> <p>For example, suppose we are generating 2D trajectories with the following procedure: we trace 2 revolutions of a circle (pattern A), and then we trace a triangle twice (pattern B). At this point with 33% chance we repeat pattern B, with 33% chance we do a figure eight twice (pattern C), or with 33% chance we return to pattern A. After pattern C we always return to pattern A.</p> <p><img src="/assets/img/circle2_triangle2_eight2_random.png" alt="random trace"/></p> <p>The figure above goes through patterns A, B, A, B, C, A, B, B, C, A (or something like that). This is like a non-deterministic finite automaton where the transitions from A to B and from C to A are deterministic, but the transitions from B are stochastic, and can go to any other state with equal probability.</p> <p>If what you want is to generate non-repeating valid sequences containing the A, B, C patterns, there is no stochastic element in the RNNs, and nothing to learn the transition probabilities.</p> <p>Here is how the MTRNN fares when trying to learn the pattern of the figure above. Because the pattern is not periodic, it must learn to reproduce the whole trace in order to make a similar figure.</p> <p><img src="/assets/img/cte_mtrnn.png" alt="cte_mtrnn"/></p> <p>On the other hand, the <a href="https://sergio-verduzco.github.io/2023/06/28/variational-autoencoder.html">variational autoencoder</a> (VAE) is capable of learning complex distributions and producing samples from them, but the standard VAE is not appropriate for modeling sequential data. Just like feedforward perceptrons, the VAE has a fixed input and output size, so taking inputs of varying lengths, or generating very long output sequences is problematic.</p> <p>If we could only combine the distribution-learning ability of the VAE with the RNNs’ ability to handle sequences…</p> <h2 id="the-variational-recurrent-neural-network-vrnn">The Variational Recurrent Neural Network (VRNN)</h2> <p>Your typical RNN works through repeated applications of a <em>cell</em>, that at time $t$ takes an input $x_t$ and the previous hidden state $h_{t-1}$. With this the cell produces a hidden state $h_{t}$ and an output $y_t$. Usually the cell is an LSTM or GRU layer. But what if our cell was a VAE?</p> <p>This is the premise of the VRNN, introduced in 2015 by <a href="https://papers.nips.cc/paper_files/paper/2015/file/b618c3210e934362ac261db280128c22-Paper.pdf">Chung et al.</a> The way it works is a combination of the procedures for using RNNs and VAEs. There are only 3 extra elements:</p> <ul> <li>There is is an intermediate extraction of features for the input $\mathbf{x}_t$ and the latent variable $\mathbf{z}_t$, which the authors claim is required for good performance.</li> <li>The normal distribution generating \(\mathbf{z}_t\) is no longer assumed to always have mean $\mathbf{0}$ and identity covariance matrix. Instead, the mean and the (diagonal) covariance matrix will depend on the previous hidden state $\mathbf{h}_{t-1}$. According to the authors, this secret sauce gives the model better representational power. The results support this claim, but the advantage it confers doesn’t seem to be dramatic.</li> <li>When generating samples the decoder outputs parameters of a distribution that generates $\mathbf{x}$, rather than providing $\mathbf{x}$ directly. This is not unusual in RNNs, but it’s not how the original VAE operated.</li> </ul> <p>If you want to generate samples using a VRNN, you begin with an initial hidden state $\mathbf{h}_0$, usually a vector with zeros. Then:</p> <ol> <li>Generate the first latent variable $\mathbf{z}_1$ in two steps. <ol> <li>Obtain the mean \(\mathbf{\mu}_{0,1}\) and covariance matrix \(\text{diag}(\mathbf{\sigma}_{0,1})\) of the prior distribution for \(\mathbf{z}\). This comes from the output of a network whose input is \(\mathbf{h}_0\), and is denoted by \(\varphi_\tau^{prior}(\mathbf{h}_0)\).</li> <li>Sample \(\mathbf{z}_1\) from the distribution \(\mathcal{N}(\mathbf{\mu}_{0,1},\text{diag}(\mathbf{\sigma}_{0,1}))\).</li> </ol> </li> <li>Extract features from \(\mathbf{z}_1\) using a network denoted by \(\varphi_\tau^{z}(\mathbf{z}_1)\).</li> <li>Obtain the first synthetic value $\mathbf{x}_1$ in two steps. <ol> <li>Feed \(\varphi_\tau^{z}(\mathbf{z}_1)\) and $\mathbf{h}_0$ into the decoder to produce parameters \(\mathbf{\mu}_{x,1}\), \(\mathbf{\sigma}_{x,1}\).</li> <li>Produce a value \(\mathbf{x}_1\) by sampling from the distribution \(\mathcal{N}(\mathbf{\mu}_{x,1}, \text{diag}(\mathbf{\sigma}_{x,1}))\).</li> </ol> </li> <li>Extract features $\varphi_\tau^x(\mathbf{x}_1)$ using a neural network.</li> <li>Update the hidden state: \(\mathbf{h}_1 = f_\theta\Big(\varphi_\tau^x(\mathbf{x}_1), \varphi_\tau^z(\mathbf{z}_1), \mathbf{h}_0 \Big)\)</li> </ol> <p>The rest of the procedure is just a repetition of the previous steps starting with the updated hidden state. The next figure illustrates this.</p> <p><img src="/assets/img/vrnn_generation.png" alt="vrnn generation"/></p> <p>The procedure for training is not very different, but now at each step $t$ you will have an input $\mathbf{x}_t$ that you will stochastically reconstruct in two broad steps by</p> <ol> <li>Obtaining a \(\mathbf{z}_t\) value using an encoder. This approximates sampling from \(p(\mathbf{z}_t \vert \mathbf{x}_{\leq t})\).</li> <li>Obtaining a $\hat{\mathbf{x}}_t$ value using a decoder. This approximates sampling from \(p(\mathbf{x}_t \vert \mathbf{z}_t)\).</li> </ol> <p>After a full sequence \(\hat{\mathbf{x}}_1, \dots, \hat{\mathbf{x}}_{T}\) has been generated, gradient descent uses an objective function that is like the sum of $T$ VAE objective functions, one for each time step: \(\mathbb{E}_{q(\mathbf{z}_{\leq T}|\mathbf{x}_{\leq T})} \left[ \sum_{t=1}^T \Big(-\text{KL}(q(\mathbf{z}_t | \mathbf{x}_{\leq T}, \mathbf{z}_{&lt; T}) \|p(\mathbf{z}_t | \mathbf{x}_{&lt; T}, \mathbf{z}_{&lt; T})) + \log p(\mathbf{x}_t | \mathbf{x}_{\leq T}, \mathbf{z}_{&lt; T}) \Big) \right]\)</p> <p>The actual flow of computations can be seen in this figure:</p> <p><img src="/assets/img/vrnn_training.png" alt="vrnn learning"/></p> <p>As with the VAE, we don’t actually calculate the expected values of the objective function. Instead we use stochastic gradient descent with individual sequences. A reconstruction error is used to reduce \(\log p(\mathbf{x}_t \vert \mathbf{x}_{\leq T}, \mathbf{z}_{&lt; T})\). Not shown in the figure above is that the reparameterization trick from VAEs is used in all sampling steps.</p> <p>Something that not clear from the “training computations” figure above is how we are going to train the network $\varphi_\tau^{prior}(\mathbf{h})$ that produces parameters for the prior distribution of $\mathbf{z}$ based on the previous hidden state $\mathbf{h}$. The answer is that during the forward passes in training we will also generate values \(\mathbf{\mu}_{0,t}, \mathbf{\sigma}_{0,t}\). The part of the loss function corresponding to the terms \(-\text{KL}(q(\mathbf{z}_t \vert \mathbf{x}_{\leq T}, \mathbf{z}_{&lt; T}) \|p(\mathbf{z}_t \vert \mathbf{x}_{&lt; T}, \mathbf{z}_{&lt; T}))\) uses \(\mathbf{\mu}_{0,t}, \mathbf{\sigma}_{0,t}\) to approximate \(p(\mathbf{z}_t \vert \mathbf{x}_{&lt; T}, \mathbf{z}_{&lt; T})\), and \(\mathbf{\mu}_{z,t}, \mathbf{\sigma}_{z,t}\) for the variational distribution \(q(\mathbf{z}_t \vert \mathbf{x}_{\leq T}, \mathbf{z}_{&lt; T})\).</p> <p>To implement the objective KL divergence part of the objective function you just need to find what this is in the case of two multivariate Gaussians. Because I didn’t want to spend time deriving this divergence I just looked it up, and found it at the last page of <a href="https://stanford.edu/~jduchi/projects/general_notes.pdf">this pdf</a> (among other places).</p> <h2 id="results">Results</h2> <p>There are quite a few elements in a VRNN, so it helps to look at previous implementations. The <a href="https://github.com/jych/nips2015_vrnn/tree/master">code</a> from the original paper uses the <a href="https://github.com/jych/cle">cle</a> framework, which is a bit dated. Instead, I found inspiration from this <a href="https://github.com/emited/VariationalRecurrentNeuralNetwork/tree/master">PyTorch implementation</a>. My own implementation can be seen <a href="https://github.com/sergio-verduzco/deep_explorations/blob/main/rnn/VRNN.ipynb">here</a>.</p> <h3 id="setting-metaparameters">Setting metaparameters</h3> <p>Basically, a VRNN will use 6 networks:</p> <ol> <li>$\varphi_\tau^x(\mathbf{x}_t)$,</li> <li>$\varphi_\tau^{prior}(\mathbf{h}_{t-1})$,</li> <li>\(\varphi_\tau^{enc}(\varphi_\tau^x(\mathbf{x}_t), \mathbf{h}_{t-1})\),</li> <li>$\varphi_\tau^z(\mathbf{x}_t)$,</li> <li>\(\varphi_\tau^{dec}(\varphi_\tau^z(\mathbf{z}_t), \mathbf{h}_{t-1})\),</li> <li>\(f_\theta \left( \varphi_\tau^x(\mathbf{x}_t), \varphi_\tau^z(\mathbf{z}_t), \mathbf{h}_{t-1} \right)\).</li> </ol> <p>To implement the VRNN you need to choose the metaparameters of these networks, and create code that applies them in the right order for inference and generation. With so many networks, choosing metaparameters is in fact one of the challenges.</p> <p>Given the task of generating the “circle-triangle-eight” pattern shown above I chose parameters with this reasoning:</p> <ul> <li>$\mathbf{h}$ has to encode the basic shape of the patterns and the the current point in the cycle. 60 units should suffice.</li> <li>The latent space needs to encode the current shape, and the amount of randomness that should be involved when choosing the next point. For this I deemed that at least two variables should be used, and to be sure I set the dimension of $\mathbf{z}$ to 10.</li> <li>$\varphi^x_\tau$ and $\varphi^z_\tau$ “extract features” from $\mathbf{x}$ and $\mathbf{z}$. It seemed fitting that the number of features should be larger than the dimensions of $\mathbf{x}$ and $\mathbf{z}$, so those features could make an “explicit” representation. I set these as 1-layer networks with 10 units for \(\varphi^x_\tau\) and 16 units for \(\varphi^z_\tau\).</li> <li>$\varphi^{prior}_\tau$ must produce the distribution for the latent variables given $\mathbf{h}$. I used a network with a 30-unit hidden layer.</li> <li>\(\varphi^{enc}_\tau\) and $\varphi^{dec}_\tau$ must produce distribution parameters based on the current hidden state and on extracted features. \(\varphi^{enc}_\tau\) is a network with a 30-unit hidden layer, whereas \(\varphi^{dec}_\tau\) uses 40 units.</li> <li>In the case of $f_{\theta}$ I used an GRU RNN with a single 60-unit layer.</li> </ul> <p>These are probably more units than necessary for the “circle-triangle-eight”, but in an initial exploration I am less concerned with overfitting, and more concerned with finding whether the network can do the task. I used smaller networks and an <a href="https://sergio-verduzco.github.io/2023/07/02/elman_rnn.html">Elman RNN</a> for the other experiments described below.</p> <p>As with the <a href="https://sergio-verduzco.github.io/2023/06/28/variational-autoencoder.html">VAE</a>, I used Mean Squared Error loss for the reconstruction error, and set an adaptive $w$ parameter to balance the magnitude of this error with the much larger output of the KL divergence between the prior and posterior $\mathbf{z}$ distributions.</p> <h3 id="a-rough-start">A rough start</h3> <p>After the code was completed the traces my network produced were random blotches. I assumed there was an error in my code, but going through each line didn’t reveal anything. After an embarrassingly long time I realized that the output of my encoder, decoder, and prior networks corresponding to means should <strong>not</strong> use ReLU units in the output layer, because then those outputs could not be negative…</p> <p>Once I modified the output of the networks, the VRNN could learn to generate the basic shapes I tested in the <a href="https://sergio-verduzco.github.io/2023/07/02/elman_rnn.html">Elman network</a> post. For example, here’s how it learned to trace a figure eight:</p> <p><img src="/assets/img/eight10_vrnn_500ep.png" alt="eight"/></p> <p>The success in generating this figure hid the fact that there was still a fundamental bug in my code. This bug came from the fact that a regular RNN cell predicts \(\mathbf{x}_{t+1} \approx \mathbf{\hat{x}}_t\) given \(\mathbf{x}_t\) at time $t$, so the reconstruction error is the sum of elements like \(\| \mathbf{\hat{x}}_t - \mathbf{x}_{t+1} \|^2\). On the other hand, the VRNN tries to reproduce \(\mathbf{x}_t \approx \mathbf{\hat{x}}_t\) at time step $t$ given \(\mathbf{h}_{t-1}\). The reconstruction error thus has terms like \(\|\mathbf{x}_t - \mathbf{\hat{x}}_t \|^2\). My mistake was to use errors like \(\| \mathbf{\hat{x}}_t - \mathbf{x}_{t+1} \|^2\). Since $\mathbf{x}_t$ and \(\mathbf{x}_{t+1}\) are usually close the model “kind of worked”, and recognizing that there was an error took me a while.</p> <p>In the Appendix below I go through the sequence of experiments and ideas I had as the model failed to work well. When things went wrong I had to think about the VRNN and why it works, so the time wasn’t totally wasted. Also, this led me to set the adaptive $w$ ratio to $\frac{w \cdot DE}{RE} \approx 10$, which seems to produce good results.</p> <h3 id="a-basic-result">A basic result</h3> <p>In this first experiment the VRNN does something new: it changes its precision depending on the context:</p> <p><img src="/assets/img/one_side_vrnn_1100ep.png" alt="one side has more precision"/></p> <p>In the original trace that we are trying to reproduce, the left side of the oval has much less variance than the right side. To some extent this is captured by the traces generated with the VRNN after enough training.</p> <h3 id="a-disappointing-result">A disappointing result</h3> <p>The next experiment I tried reproducing a stochastic “eye” pattern.</p> <p>In this pattern the wide oval is traced with probability 2/3, and the thin oval with probability 1/3, with transitions happening at the top, near the x=0, y=1 coordinate.</p> <p><img src="/assets/img/eye_vrnn_4800ep.png" alt="eye pattern generation"/></p> <p>As can be seen, after 4800 epochs the VRNN could not replicate the process generating this pattern. Curiously, the reconstruction loss did get really low, which led me to wonder how well this network could reconstruct the original trace, mostly working as an autoencoder. If you reconstruct the trace using the “inference” mechanism (the one using during training) the reconstruction is quite close to the original.</p> <p><img src="/assets/img/eye_vrnn_4800ep_infer.png" alt="inference reconstruction of eye pattern"/></p> <p>So with the right context the network can produce the pattern, but the generation mechanism is at this point insufficient.</p> <h2 id="appendix-experiments-with-a-predictive-vrnn">Appendix: experiments with a “predictive” VRNN</h2> <p>As described above, my VRNN implementation calculated the reconstruction loss incorrectly. In code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x, pmu, psig, mu, sig = vrnn(coords[:-1])
RE = mse_loss(x, coords[1:])
</code></pre></div></div> <p>So the loss function used a shift in the input coordinates, as is common in autoregressive models. But this should not be done with the VRNN.</p> <p>So I had just reproduced the figure 8 as shown previously. The interesting part was whether I would succeed with the “circle triangle eight” pattern.</p> <p>Here is how it looked (1000 points are generated for each trajectory below):</p> <p><img src="/assets/img/circle2_triangle2_eight2_nz10_3000epochs.png" alt="initial vrnn"/></p> <p>The impression I had is that the transition points between patterns are far and few, so it is hard for the network to learn proper $\mathbf{z}_t$ representations. Basically, it is easier to just learn a single trajectory that approaches the points of the single example I provided for the network to learn.</p> <p>The first modification I used was to use GRU units for the $f_\theta$ network, which may help remember back to the transition points. I hoped that this and a large number of training epochs would do the trick. It did not, so I was left to wonder what was the problem.</p> <p>I wanted the decoder to produce 3 attractors (circle, triangle, eight), and to switch between them based on the latent variable $\mathbf{z}_t$, which would potentially change its value when two cycles of the B pattern (the triangle) were completed. Instead I found a single attractor contorting its shape to match the original trace.</p> <p>It didn’t seem like the latent variable was learning the transition points between patterns, and this should not be so surprising considering how sparse they are. My next move was to increase the training data, introducing 8 traces, each one with 5 to 10 transitions between patterns. This, together with GRU units, a VRNN with large layers, and enough training epochs should do the trick…</p> <p><img src="/assets/img/cte_gru_8tp_6800ep.png" alt="8 training patterns"/></p> <p>No, it didn’t do the trick. What now?</p> <p>One thing I noticed is that the loss had become really small ($\approx$ 0.0001), both for the reconstruction error, and for $w$ times the KL divergence. The loss in the distribution of $\mathbf{z}$ was small, and yet the performance was poor. Perhaps setting $w$ so that $\frac{w \cdot DE}{RE} \approx 1$ was not so good in this case (see the VAE post). As a first variation I tried to set the initial $w=0$ value, and then adjust $w$ adaptive to approach the ratio $\frac{w \cdot DE}{RE} \approx 10$.</p> <p>Another observation is that the circle-triangle-eight trajectory with two cycles of each shape is a challenging figure to trace. Given my lack of success, it may be better to try a simpler pattern, which I did.</p> <p>For the next round of attempts I used the following “eye” pattern:</p> <p><img src="/assets/img/long_eye.png" alt="eye pattern"/></p> <p>In this pattern when the pen is at the top, with probability 2/3 a wide oval will be traced, and with probability 1/3 a thin oval will be traced. Around 30 of these transitions were included in a single figure. Results from learning this pattern can be seen below.</p> <p><img src="/assets/img/eye_vrnn_gru_1100ep.png" alt="eye results"/></p> <p>A this point I had this thought: a network that only traces the wide oval will reduce the loss just as much as a network that traces the wide oval with probability 2/3, and the thin oval with probability 1/3.</p> <p>Because generation is stochastic, a “perfect” model only has probability 5/9 of matching the training data on any given cycle: the model matches the training data when 1) both are wide (with probability 4/9), and 2) both are thin (with probability 1/9). On the other hand, a “lazy” model that only traces the wide ovals will match the training data 66% of the time. This argument ignores differences in the phase caused by the thin oval being smaller, but those should be similar for both models.</p> <p>In light of this, the reconstruction loss is not sufficient for learning the type of model we desire. Something must pressure the discovery of “transitions between patterns” at particular points. The question is why \(\text{KL}(q(\mathbf{z}_t \vert \mathbf{x}_{\leq T}, \mathbf{z}_{&lt; T}) \|p(\mathbf{z}_t \vert \mathbf{x}_{&lt; T}, \mathbf{z}_{&lt; T}))\), should be able to do this. As far as I can tell, it can’t. You really need \(\mathbb{E}_{q(\mathbf{z}\leq T \vert \mathbf{x}\leq T)} \Big[ \text{KL}(q(\mathbf{z}_t \vert \mathbf{x}_{\leq T}, \mathbf{z}_{&lt; T}) \|p(\mathbf{z}_t \vert \mathbf{x}_{&lt; T}, \mathbf{z}_{&lt; T})) \Big]\), or in other words, a long number of training epochs with small learning rates.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post I try to explain the ideas behind the Variational Recurrent Neural Network, and convey the experiences from implementing one with PyTorch.]]></summary></entry><entry><title type="html">Multiple-Timescales Recurrent Neural Networks (MTRNNs)</title><link href="https://sergio-verduzco.github.io/blog/2023/mtrnn/" rel="alternate" type="text/html" title="Multiple-Timescales Recurrent Neural Networks (MTRNNs)"/><published>2023-07-03T00:00:00+09:00</published><updated>2023-07-03T00:00:00+09:00</updated><id>https://sergio-verduzco.github.io/blog/2023/mtrnn</id><content type="html" xml:base="https://sergio-verduzco.github.io/blog/2023/mtrnn/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post is a continuation of my previous post on <a href="https://sergio-verduzco.github.io/2023/07/02/elman_rnn.html">Elman networks</a>.</p> <p>The MTRNN is a network architecture capable of learning long time dependencies more effectively than an Elman network. This architecture is described in <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000220#s4">Yamashita and Tani 2010</a>, <strong>Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment</strong>.</p> <p>This is very similar to a cascade of Elman or Jordan recurrent networks, with the key difference that the networks are now considered to operate in continuous time, and the units in the higher levels have slower (larger) time constants.</p> <p>I made a simple adaptation of the network in Yamshita and Tani 2010 for the problem of learning traces (sequences of 2D coordinates). The basic architecture is in the next figure.</p> <p><img src="/assets/img/mtrnn.png" alt="mtrnn adaptation"/></p> <p>As in the Elman RNN post, the input to the network is a sequence of $(x, y)$ coordinates, and the output is a prediction of the coordinates $(x’, y’)$ at the next time step. L0 is a fully-connected layer, using $tanh$ nonlinearities in this implementation. L1 is a linear layer. H0 and H1 are Elman RNN cells with fast and slow timescales, respectively.</p> <p>A difference with the Yamashita and Tani 2010 network is that in here outputs come from a linear layer, rather than a softmax layer. Moreover, H0 and H1 each consist of a single layer, and they update after applying the $tanh$ nonlinearity. In other words, if $\mathbf{h}_i$ denotes the activation vector of the units in hidden layer $i$, the update steps are described by:</p> \[\mathbf{h}_i^* = \tanh \left( \mathbf{W_i I_i} \right)\] \[\mathbf{h}_i \leftarrow \left( 1 - \frac{1}{\tau_i} \right) \mathbf{h}_i + \frac{1}{\tau_i} \mathbf{h}_i^*\] <p>Where $\mathbf{W}_i$ and $\mathbf{I}_i$ are respectively the weight matrix and input vector for hidden layer $i$. As a curious note, initially I made a mistake when wrting the equations to update the activity, so the update was: $\mathbf{h}_i \leftarrow \left( 1 - \frac{1}{\tau_i} \right) \mathbf{h}_i^*$. The network with this update rule could still learn to approximate circles, triangles, and with some work even figure eights. This is probably because the network with these update equations is like two stacked Elman networks with some error in the activation. It serves to illustrate a valuable lesson, so I threw this result in an Appendix at the bottom of this post.</p> <p>Going back to the network with the correct equations, I used layer sizes of 10, 40, 8 for L0, H0, and H1, respectively. The time constant for H0 was 4, and the one for H1 was 80. The optimizer was Adam with time constant 3e-4.</p> <h2 id="results">Results</h2> <p>Like the Elman network, the MTRNN could learn to trace circles, triangles, and infinity symbols. This is very similar to the case with the Elman network, so I’ll only show a single result.</p> <p>In this case it can be seen that the self-intersection does not prevent learning. The attractors produced by the Elman network seemed “cleaner”, however. It is possible that the MTRNN, with its higher memory capacity, was learning to reproduce some of the quirks in the hand-drawn image (overfitting).</p> <p><img src="/assets/img/eight10_mtrnn.png" alt="figure eight results"/></p> <p>Training for a longer period suggests that this may be the case. The attractors produced by the Elman network seemed to be limit cycles whose period coincided with the periodicity of the trajectory in the drawing. In the case of the MTRNN, the attractor seems to be a limit cycle that spans several loops (the original trace contains 10 loops).</p> <p><img src="/assets/img/eight10_mtrnn_long_train.png" alt="figure eight results 2"/></p> <p>The more interesting part is the trace where the side switches every few revolutions, requiring a longer memory of the trajectory. The result for this case can be seen below.</p> <p><img src="/assets/img/switch_circle2_circle2_6600_mtrnn.png" alt="switch results"/></p> <p>After 6600 epochs the trajectory switched sides while still making a loop, which made the plot messy. But clearly this was not separate attractors on each side, as in the case of the Elman network. There was a single attractor that for initial states with zeros in the hidden layers started its trajectory by looping on the left side, and then shifted to the right. Partial trajectories for a few initial states can be seen in the figure below.</p> <p><img src="/assets/img/switch_circle2_circle2_6600_mtrnn_ics.png" alt="switch results 2"/></p> <h2 id="some-thoughts">Some thoughts</h2> <p>I did very little in terms of optimizing these hyperparameters but I still came out wondering whether there is a principled way to obtain good values for the various time constants. Perhaps along the lines of extracting the Fourier coefficients of the incoming inputs. Large coefficients for slow frequencies would cause time constants to increase, etc.</p> <h2 id="appendix-incorrect-equations-but-so-so-results">Appendix: Incorrect equations, but so-so results</h2> <p>Before I realized that my MTRNN had the wrong equations I tried to teach it a pattern consisting of 5 circular revolutions on the left, followed by 5 revolutions on the right, then back to the left, and so on.</p> <p>The network was reducing the error during training, but it became clear that when generating trajectories it didn’t learn to complete 5 revolutions on one side, and then go to the other side. This prompted me to increase the number of epochs. After 10000 epochs the error reached a very small value (~8e-05), and the results can be seen below:</p> <p><img src="/assets/img/switch_circle5_circle5_long_train.png" alt="traces_long_train"/> <img src="/assets/img/switch_circle5_circle5_long_train2.png" alt="many initial conditions"/></p> <p>It can be observed that the network learned to trace multiple revolutions on the left, before finally settling on the limit-cycle attractor on the right, from which it can’t escape. This is still not the desired pattern, but an interesting approximation nonetheless.</p> <p>One thing this example shows is that the ability of backpropagation to reduce the loss can mask errors in the code. In the words of Andrej Karpathy:</p> <blockquote> <p>It is a depressing fact that your network will typically still train okay…</p> </blockquote> <p>That’s one reason he recommends an evaluation skeleton and many sanity checks in his <a href="https://karpathy.github.io/2019/04/25/recipe/">Recipe for Training Neural Networks</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Capturing patterns at different timescales]]></summary></entry><entry><title type="html">The Elman RNN</title><link href="https://sergio-verduzco.github.io/blog/2023/elman_rnn/" rel="alternate" type="text/html" title="The Elman RNN"/><published>2023-07-02T00:00:00+09:00</published><updated>2023-07-02T00:00:00+09:00</updated><id>https://sergio-verduzco.github.io/blog/2023/elman_rnn</id><content type="html" xml:base="https://sergio-verduzco.github.io/blog/2023/elman_rnn/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This relates the experience of implementing an Elman network for modeling sequential information.</p> <p>The Elman network was introduced by Jeffrey L. Elman in a 1990 paper titled <a href="https://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1">Finding Structure in Time</a>. It uses ideas very similar to those introduced by Michael I. Jordan in a 1986 <a href="https://cseweb.ucsd.edu/~gary/PAPER-SUGGESTIONS/Jordan-TR-8604-OCRed.pdf">technical report</a>.</p> <p>The Jordan and Elman models address a basic limitation of multilayer perceptrons: the input size and number of layers (processing steps) is fixed. This can be a problem when processing sequential inputs where the input size is variable (e.g. speech, sound, time series …). Jordan’s idea was to go go through the inputs one at a time, and having a <strong>state</strong> layer that maintains information about the previous inputs up to this point. To implement this state, the inputs to the network are expanded with a <em>state</em> layer whose inputs are the outputs of the network at the previous time step. In other words, the inputs to the hidden layer at time $t$ include the outputs of the network at time $t-1$.</p> <p>The Elman network, also known as the SRN or <a href="https://web.stanford.edu/group/pdplab/pdphandbook/handbookch8.html">Simple Recurrent Network</a> is very similar. The difference is that the input to the hidden layer, instead of receiving a copy of the network’s output, receives a copy of the hidden layer’s activity at the previous time step.</p> <p>The task for this implementation was to learn to recreate a set of hand-drawn traces: <img src="/assets/img/figures.png" alt="the figures to be traced"/></p> <p>These traces were drawn by hand and exported into the <code class="language-plaintext highlighter-rouge">.svg</code> format. I wrote a <a href="https://github.com/sergio-verduzco/deep_explorations/blob/main/rnn/coords_from_svg.ipynb">function</a> to extract a sequence of $(x,y)$ coordinates from the svg file, which were stored in Numpy array with roughly 1000 rows and 2 columns. The task of the network was to learn this sequence of traces, but due to the noisy nature of hand drawing the network must create limit cycle attractors that roughly follow the mean trajectory of the lines.</p> <p>The circle and the triangle are the simplest examples for one reason: to predict the coordinates at the next time point all you require are the $(x, y)$ coordinates at the current time. This is especially true when $(x, y)$ is close to the “mean trajectory”. In theory a multilayer perceptron could learn to trace these shapes.</p> <p>The figure eight (infinity symbol) self intersects, and at the intersection point you need some memory of the previous points in order to predict what is the next coordinate in the trajectory. This pattern thus requires some memory, and the Elman network seems just right for the task.</p> <p>The figure on the bottom right is the most challenging one. The trace begins by doing circle on the left. After 2.5 circular revolutions the trace moves to the right and begins another circle. Every 2.5 revolutions the circle is switched. Learning to predict the next point requires a much longer memory now, enough to remember how many circular revolutions have been completed on the current side. Such a memory requirement strains the memory capacity of the Elman network, and motivates the introduction of the Multiple-Timescales RNN, described in a subsequent <a href="https://sergio-verduzco.github.io/2023/07/03/mtrnn.html">blog post</a>.</p> <h2 id="results">Results</h2> <p>I implemented a basic Elman RNN using Python and Pytorch. Source code is <a href="https://github.com/sergio-verduzco/deep_explorations/blob/main/rnn/Elman_network.ipynb">here</a>. For the purpose of this exploration I used a hidden layer with 50 units, <em>tanh</em> nonlinearity, the Adam optimizer, and a learning rate of 3e-4.</p> <p>First, let’s look at the circle.</p> <p><img src="/assets/img/circle10_elman.png" alt="circle results"/></p> <p>To generate the panel on the right the trained Elman network was given an initial coordinate ($x=0, y=0$), and an initial state for the hidden layer (all activity set to zero). The trace on the panel was created by recursively feeding the output at time step $t$ as the input for time step $t+1$. We can see that after 300 epochs of training we have a nice limit cycle attractor as emerged.</p> <p>A few more initial conditions can be seen in this figure:</p> <p><img src="/assets/img/circle10_elman_ic.png" alt="circle initial conditions"/></p> <p>The pink dots in the figure represent the initial coordinates given to the Elman network; in this case a 4 x 4 grid of values. The cyan dots show the first point produced for all the 16 initial conditions. The large discontinuous jump from the pink to the cyan dots is a reminder that the recurrent trace generation done here is a discrete process, not a continuous one. Moreover, this plot should not be interepreted as a phase diagram. As a dynamical system, the recurrent process is 52-dimensional, so its flow can’t entirely be captured in 2 dimensions. All the trajectories shown here start from the point where the hidden units have zero activity.</p> <p>With those caveats being said, it is clear from the figure that a large number of initial conditions are attracted to the circular trajectory. The case of learning to trace a triangle is very similar, as can be observed in the figures below.</p> <p><img src="/assets/img/triangle10_elman.png" alt="triangle results"/></p> <p><img src="/assets/img/triangle10_elman_ic.png" alt="triangle results"/></p> <p>The infinity symbol took many more epochs to learn:</p> <p><img src="/assets/img/infty10_elman.png" alt="infinity results"/></p> <p><img src="/assets/img/infty10_elman_ic.png" alt="infinity results"/></p> <p>Initially the network settled into a roughly circular attractor, and it took many cycles of training before this trajectory could be deformed into a self-intersecting shape.</p> <p>Finally, I attempted to learn to predict the trajectory where the circular motions switch sides every 2.5 cycles. As can be seen below, the network tried to approximate the shape, and even seems to have learned separate attractors for the circles on the right and the left, but trajectories settled on a single circle and didn’t switch sides.</p> <p><img src="/assets/img/switch_circle2_circle2_4200_elman.png.png" alt="switch results"/></p> <h2 id="bonus-round-lstm">Bonus round: LSTM</h2> <p>The problem here is that we need information about the trajectory’s history far in the past. This is precisely the type of problem that <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTMs</a> are meant to solve. As a bonus, I quickly replaced the Elman RNN for an LSTM network. There was no parameter tuning whatsoever, so consider that when seeing the next figure:</p> <p><img src="/assets/img/switch_circle2_lstm.png" alt="LSTM results"/></p> <p>My impression is that LSTM can indeed solve this problem, although the current parameters aren’t particularly effective. I was able to run 10200 epochs because the Pytorch LSTM implementation is seriously fast.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">The variational autoencoder from scratch: an exercise in balance</title><link href="https://sergio-verduzco.github.io/blog/2023/variational-autoencoder/" rel="alternate" type="text/html" title="The variational autoencoder from scratch: an exercise in balance"/><published>2023-06-28T00:00:00+09:00</published><updated>2023-06-28T00:00:00+09:00</updated><id>https://sergio-verduzco.github.io/blog/2023/variational-autoencoder</id><content type="html" xml:base="https://sergio-verduzco.github.io/blog/2023/variational-autoencoder/"><![CDATA[<p>In this post I wrote some thoughts on what the Variational Autoencoder (VAE) is supposed to do, and on ideas I got while programming it from scratch.</p> <p>A lot of these thoughts were motivated by reading <a href="https://arxiv.org/pdf/1606.05908.pdf">Doersch 2017</a>, which was my entry point to VAEs.</p> <p>Source code for my VAE implementation (which is not particularly clean) is located <a href="https://github.com/sergio-verduzco/deep_explorations/tree/main/variational">here</a>. Source code from people who know what they’re doing can be seen <a href="https://github.com/karpathy/examples/blob/master/vae/main.py">here</a>.</p> <h2 id="preamble">Preamble</h2> <p>A lot of the work in statistical machine learning is focused on learning a distribution $p(\mathbf{x})$ based on a collection of examples ${ \mathbf{x}_1, \dots, \mathbf{x}_n }$. These examples could be things like faces or sentences, and an interesting thing about having $p(\mathbf{x})$ is that then you can <em>sample</em> from that distribution, to generate synthetic faces or sentences.</p> <p>Some people get excited about learning a distribution and sampling from it. Perhaps because in some sense this captures the process that generates the samples, so the distribution models an aspect of the world and its uncertainty. The problem is that the sample space is just too big in the interesting cases. How many 512x512 RGB images are possible?</p> <p>An approach to make distributions tractable is to extract <em>latent variables</em>. Ideally, these variables are related to the process that generates the $\mathbf{x}$ data points, and can encode their distribution with dramatically reduced dimensionality. For example, if the data points are images of digits, a single variable with the identity of the digit (0-9) would go a long way in capturing the relevant information in the image.</p> <p>Working with latent variables (denoted here by $\mathbf{z}$) has at least two big challenges. The first is deciding what the variable will encode. Which features can capture the information in the training data? The second challenge is to obtain the distribution $p(\mathbf{z})$. Obtaining this distribution is important because once you know it you can take samples of $\mathbf{z}$, and with with the help of a <em>decoder</em> (mapping values of $\mathbf{z}$ to their corresponding value of $\mathbf{x}$) you can generate synthetic data, as if you were sampling from $p(\mathbf{x})$.</p> <h2 id="the-vae">The VAE</h2> <table> <tbody> <tr> <td>The <a href="https://arxiv.org/abs/1312.6114">variational autoencoder</a> is an architecture capable of learning the latent variables $\mathbf{z}$ that correspond to a given input $\mathbf{x}$ (in other words, approximately learning the distribution $p(\mathbf{z}</td> <td>\mathbf{x})$), and of producing a decoder network that, given $\mathbf{z}$, yields a corresponding value of $\mathbf{x}$. Moreover, the $\mathbf{z}$ variables it learns are such that $p(\mathbf{z})$ is close to a multivariate normal distribution, so we can sample $\mathbf{z}$ values and feed them to the decoder in order to produce synthetic data!</td> </tr> </tbody> </table> <p>I’ll skip all the math (there are better explanations out there), and jump into what the VAE is computationally, what is the intuition, and how you train it.</p> <p>The VAE is this stochastic machine:</p> <p><img src="/assets/img/vae.png" alt="vae architecture"/></p> <p>This machine takes the original high-dimensional input $\mathbf{x}$ (e.g. images), and stochastically produces a “reconstructed” version of $\mathbf{x}$, denoted by $\hat{\mathbf{x}}$.</p> <p>The encoder is some neural network that receives $\mathbf{x}$ and outputs two vectors $\mu(\mathbf{x}), \text{diag}\left(\Sigma^{1/2}(\mathbf{x})\right)$. Each of these two vectors has $n_z$ elements, with $n_z$ being the number of latent variables. $\mu(\mathbf{x})$ and $\text{diag}\left(\Sigma^{1/2}(\mathbf{x})\right)$ are the parameters of a multivariate normal distribution that will be used to stochastically generate $\mathbf{z}$ by sampling from it. This normal distribution is assumed to have a diagonal covariance matrix $\Sigma$, so we only need $n_z$ elements to represent it using the vector $\text{diag}\left(\Sigma^{1/2}\right)$. The vector $\mu$ contains the means of the distribution.</p> <p>Sampling from the multivariate normal during training is done in a sneaky way. Rather than sampling directly from $\mathcal{N}(\mu, \Sigma)$ we sample a vector $\mathbf{\varepsilon}$ from a standard multivariate normal distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$ (zero mean and identity covariance matrix). Then the sample is produced as \(\mathbf{z} = \mu(\mathbf{x}) + \mathbf{\varepsilon} * \Sigma^{1/2}(\mathbf{x})\) What this achieves is to make the path of computations from $\mathbf{x}$ to $\hat{\mathbf{x}}$ entirely differentiable, which allows us to do backpropagation using the $|\mathbf{x} - \hat{\mathbf{x}}|^2$ <em>reconstruction error</em>. Error measures different from mean-squared error may be used, but the idea is the same. Had we sampled directly from $\mathcal{N}(\mu(\mathbf{x}), \Sigma^{1/2}(\mathbf{x}))$ the non-differentiable random sampling part would have blocked backpropagation of gradients. This sneaky sampling is known as the <em>reparameterization trick</em>.</p> <p>The decoder is a neural network that takes $\mathbf{z}$ and outputs $\hat{\mathbf{x}}$.</p> <p>At this point we are in position to train both the decoder and the encoder using backpropagation and the reconstruction error. But if we only use this error then the VAE will still not allow us to generate synthetic outputs by sampling $\mathbf{z}$. Why? Because the distribution of $\mathbf{z}$ that we use for training is different (has different $\mu, \Sigma$ parameters) for every value of $\mathbf{x}$. Which distribution can use use for sampling $\mathbf{z}$ to generate data?</p> <p>The solution is to train the encoder so that $\mathbf{z}$ has a known, simple distribution $p(\mathbf{z})$ that allows sampling. In the most common version of the VAE we assume that the true distribution $p(\mathbf{z})$ is $\mathcal{N}(\mathbf{0}, \mathbf{I})$. Since $p(\mathbf{x})$ will usually not be anything like a standard normal, it is really unlikely that the output $\mu, \Sigma$ of the encoder will be anything like a standard normal distribution when the encoder’s parameters are being adjusted only to reduce the reconstruction error.</p> <p>In reality the encoder will produce an output with distribution $q(\mathbf{z})$. We would like to modify the weights of the decoder so not only is the reconstruction error is minimized, but also $q(\mathbf{z})$ gets close to a standard normal distribution. Thus, the loss function for the encoder needs another term that quantifies the difference between $q(\mathbf{z})$ and $\mathcal{N}(\mathbf{0}, \mathbf{I})$. When you want to quantify the difference between two distributions the usual measure is the Kullback-Leibler divergence, and this is what the VAE uses.</p> <table> <tbody> <tr> <td>Thus you’ll have a term $\text{KL}[q(\mathbf{z}) | p(\mathbf{z})]$ in the decoder’s loss, but estimating $q(\mathbf{z})$ is still computationally expensive, so what you’ll do is to use $\text{KL}[q(\mathbf{z}</td> <td>\mathbf{x}) | p(\mathbf{z})]$ repeatedly. In other words, for each point $\mathbf{x}$ in the training data you’ll produce gradients so the encoder produces values $\mu(\mathbf{x}), \Sigma^{1/2}(\mathbf{x})$ closer to $\mathbf{0}, \mathbf{1}$. This tendency to produce values of $\mu, \Sigma$ that are just vectors with zeros and ones must be balanced with the requirement of $\mu(\mathbf{x}), \Sigma(\mathbf{x})$ still maintaining information about $\mathbf{x}$, so the decoder can reconstruct it.</td> </tr> </tbody> </table> <h2 id="results">Results</h2> <p>I wrote <a href="https://github.com/sergio-verduzco/deep_explorations/tree/main/variational">a version of the VAE</a> based on equation 7 in <a href="https://arxiv.org/pdf/1606.05908.pdf">Doersch 2017</a>. In particular: \(\text{KL}(q(\mathbf{z|\mathbf{x}}) \| \mathcal{N}(\mathbf{0}, I)) = \frac{1}{2}\left(\text{tr}(\Sigma(\mathbf{x}) + \mu(\mathbf{x})^T \mu(\mathbf{x}) -k - \text{log det}(\Sigma(\mathbf{x}))) \right)\)</p> <p>The loss function was the sum of the reconstruction error plus this divergence \(\mathcal{L} = \|\mathbf{x} - \mathbf{\hat{x}} \|^2 + \text{KL}(q(\mathbf{z|\mathbf{x}}) \| \mathcal{N}(\mathbf{0}, I)) \equiv RE + DE\)</p> <p>where RE stands for “Reconstruction Error”, and DE stands for “Distribution Error”. Notice that $q$ does not depend on the parameters of the decoder. The derivative of the DE only affects the parameters of the encoder, whereas the derivative of RE affects both encoder and decoder.</p> <p>I trained the encoder to produce $\mu, \text{diag}(\Sigma)$ values. Unbeknownst to me, most implementations of the VAE follow the original Kingma and Welling 2013 derivation, in which the encoder outputs the logarithm of $\mu$ and $\Sigma$. This shouldn’t affect the direction of backpropagation, but in practice it has the effect that it changes the relative magnitude of the two errors in the loss function.</p> <p>The first time that I trained my VAE to produce images of digits based on the MNIST dataset all the samples would produce the same image, which looked a bit like an “average number”.</p> <p><img src="/assets/img/vae_2D_unbalanced.png" alt="vae 2D no balance"/></p> <p>The VAE for this image has a 2D latent variable $\mathbf{z}$, and there is a 10x10 grid of $\mathbf{z}$ values where each dimension ranges from -2 to 2. Training proceeded for 10 epochs.</p> <p>How to fix this?</p> <p>A first clue was that the reconstruction error was more than one order of magnitude smaller than the distribution error. My guess is that reducing the reconstruction error too aggressively caused training to enter an attractor from which it couldn’t escape, because the escape routes involved increasing RE for a few iterations.</p> <p>The easy fix was to modify the loss function as \(\mathcal{L} = RE + w \cdot DE\) where $w=0.001$. This allowed reconstruction of the digits.</p> <p><img src="/assets/img/vae_2D_balanced1.png" alt="vae 2D balance"/></p> <p>Finding a good value of $w$ was quite time consuming. I decided to try to automate this process using this criterion: on average, $RE$ should have a similar magnitude to $w RE$. In other words, at every iteration slightly modify $w$ so that $\frac{wDE}{RE} \approx 1$. The ratio of 1 is an arbitrary quantity, but worked well for this example.</p> <p><img src="/assets/img/vae_2D_balanced2.png" alt="vae 2D balance 2"/></p> <p>What I did was to start with $w=0$, and then on every minibatch to adust its value as \(\Delta w = \alpha (RE - wDE)\) with $\alpha = 10^{-5}$.</p> <p>I did’t know it at the time, but what I had conjured was a variation of the <strong>KL cost annealing</strong> introduced in <a href="https://arxiv.org/abs/1511.06349">this paper</a> (Bowman et al. 2015, “Generating sentences from a continuous space”).</p> <h3 id="bonus">Bonus:</h3> <p>Using a single latent variable we get</p> <p><img src="/assets/img/vae_1D.png" alt="vae 1D"/></p> <p>A lot of information gets stored in a single $z$ value!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post I wrote some thoughts on what the Variational Autoencoder (VAE) is supposed to do, and on ideas I got while programming it from scratch.]]></summary></entry></feed>